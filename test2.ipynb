{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the XML file\n",
    "xml_file = 'defects.xml'  # Replace with your XML file path\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from XML\n",
    "data = []\n",
    "for defect in root.findall('Defect'):\n",
    "    image_name = defect.get('name')\n",
    "    labels = {child.tag: int(child.text) for child in defect}\n",
    "    data.append([image_name] + list(labels.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename',\n",
       " 'Background',\n",
       " 'Crack',\n",
       " 'Spallation',\n",
       " 'Efflorescence',\n",
       " 'ExposedBars',\n",
       " 'CorrosionStain']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating DataFrame\n",
    "column_names = ['filename'] + [child.tag for child in root[0]]\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator for data augmentation\n",
    "dataset_path = 'defects'\n",
    "datagen = ImageDataGenerator(rescale=1/255, rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
    "                             shear_range=0.1, zoom_range=0.1, horizontal_flip=True)\n",
    "\n",
    "def generate_data_generator_for_two_images(gen, df, directory, batch_size):\n",
    "    genX1 = gen.flow_from_dataframe(dataframe=df, directory=directory, x_col='filename',\n",
    "                                    y_col=column_names[1:], class_mode='raw', batch_size=batch_size,\n",
    "                                    target_size=(224, 224), seed=7)\n",
    "    while True:\n",
    "        X1i = genX1.next()\n",
    "        yield X1i[0], X1i[1]\n",
    "\n",
    "train_generator = generate_data_generator_for_two_images(datagen, train_df, dataset_path, batch_size=4)\n",
    "validation_generator = generate_data_generator_for_two_images(datagen, val_df, dataset_path, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               12845568  \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,563,334\n",
      "Trainable params: 27,563,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 1 VGG16 - Non trainable base model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Load VGG16 as the base model\n",
    "base_model_vgg16 = VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the layers of VGG16\n",
    "for layer in base_model_vgg16.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Building a custom model for multi-label classification based on VGG16\n",
    "model = Sequential([\n",
    "    base_model_vgg16,  # Using VGG16 as the base\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(column_names) - 1, activation='sigmoid')  # Output layer with sigmoid activation for multi-label classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "398/398 [==============================] - 999s 3s/step - loss: 0.5857 - accuracy: 0.2731 - val_loss: 0.5407 - val_accuracy: 0.3409\n",
      "Epoch 2/150\n",
      "398/398 [==============================] - 929s 2s/step - loss: 0.5468 - accuracy: 0.3160 - val_loss: 0.5439 - val_accuracy: 0.3595\n",
      "Epoch 3/150\n",
      "398/398 [==============================] - 921s 2s/step - loss: 0.5463 - accuracy: 0.3103 - val_loss: 0.5369 - val_accuracy: 0.4091\n",
      "Epoch 4/150\n",
      " 95/398 [======>.......................] - ETA: 10:49 - loss: 0.5453 - accuracy: 0.3447"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator, \n",
    "                    epochs=150, \n",
    "                    validation_data=validation_generator, \n",
    "                    steps_per_epoch=len(train_df) // 16,  # Adjusted steps per epoch for batch size of 16\n",
    "                    validation_steps=len(val_df) // 16,  # Adjusted validation steps for batch size of 16\n",
    "                    batch_size=16)  # Setting batch size to 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               12845568  \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,563,334\n",
      "Trainable params: 12,848,646\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Save and load the model\n",
    "model.save('base_model_vgg16-200.h5')\n",
    "model_vgg16 = tf.keras.models.load_model('base_model_vgg16-200.h5')\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/WJ/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Load the model\n",
    "model = load_model('base_model_vgg16-200.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the model\n",
    "img_path = 'image_0001599_crop_0000006.png'  # Replace with path to your test image\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = np.reshape(img, [1, 224, 224, 3])\n",
    "img = img / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mimg\u001b[49m)\n\u001b[1;32m      2\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m [column_names[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(preds[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Concrete Damage Types:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_classes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "preds = model.predict(img)\n",
    "predicted_classes = [column_names[i+1] for i, prob in enumerate(preds[0]) if prob > 0.5]\n",
    "print(\"Predicted Concrete Damage Types:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
      "Predicted Concrete Damage Types for image_0001595_crop_0000009_copy00.png : ['Spallation', 'ExposedBars', 'CorrosionStain']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "Predicted Concrete Damage Types for image_0001599_crop_0000008.png : ['Spallation', 'ExposedBars', 'CorrosionStain']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Predicted Concrete Damage Types for image_0001598_crop_0000001.png : ['Crack']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Predicted Concrete Damage Types for image_0001599_crop_0000006.png : ['Spallation', 'ExposedBars', 'CorrosionStain']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Directory containing your images\n",
    "image_dir = \"TestImages\"\n",
    "\n",
    "# List all files in the directory\n",
    "image_files = os.listdir(image_dir)\n",
    "\n",
    "for image_file in image_files:\n",
    "    # Read the image\n",
    "    img_path = os.path.join(image_dir, image_file)\n",
    "    img = cv2.imread(img_path)  # Adjust based on your image reading library\n",
    "\n",
    "    # Preprocess the image if necessary (resizing, normalization, etc.)\n",
    "    img = cv2.resize(img, (224, 224))  # Resize the image to (224, 224)\n",
    "\n",
    "    # Make predictions\n",
    "    preds = model.predict(img.reshape(1, *img.shape))  # Assuming your model expects a batch dimension\n",
    "    predicted_classes = [column_names[i+1] for i, prob in enumerate(preds[0]) if prob > 0.5]\n",
    "\n",
    "    # Print the predictions\n",
    "    print(\"Predicted Concrete Damage Types for\", image_file, \":\", predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_training_output(output):\n",
    "    lines = output.strip().split('\\n')\n",
    "    history = {'accuracy': [], 'val_accuracy': [], 'loss': [], 'val_loss': []}\n",
    "\n",
    "    for line in lines:\n",
    "        try:\n",
    "            # Check if the line contains epoch information\n",
    "            if 'loss:' in line and 'accuracy:' in line:\n",
    "                # Split the line by spaces and filter out empty strings\n",
    "                parts = [part for part in line.split(' ') if part]\n",
    "\n",
    "                # Find and extract the metrics\n",
    "                loss_idx = parts.index('loss:') + 1\n",
    "                accuracy_idx = parts.index('accuracy:') + 1\n",
    "                val_loss_idx = parts.index('val_loss:') + 1\n",
    "                val_accuracy_idx = parts.index('val_accuracy:') + 1\n",
    "\n",
    "                # Add the values to the history dictionary\n",
    "                history['loss'].append(float(parts[loss_idx]))\n",
    "                history['accuracy'].append(float(parts[accuracy_idx]))\n",
    "                history['val_loss'].append(float(parts[val_loss_idx]))\n",
    "                history['val_accuracy'].append(float(parts[val_accuracy_idx]))\n",
    "        except (IndexError, ValueError) as e:\n",
    "            print(f\"Error parsing line: '{line}'\")\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# Your training data here (as a multiline string)\n",
    "training_output = \"\"\"\n",
    "Epoch 1/200\n",
    "1594/1594 [==============================] - 683s 428ms/step - loss: 0.5018 - accuracy: 0.4850 - val_loss: 0.4086 - val_accuracy: 0.5933\n",
    "Epoch 2/200\n",
    "1594/1594 [==============================] - 680s 427ms/step - loss: 0.4264 - accuracy: 0.5955 - val_loss: 0.3577 - val_accuracy: 0.7090\n",
    "Epoch 3/200\n",
    "1594/1594 [==============================] - 672s 421ms/step - loss: 0.4220 - accuracy: 0.5745 - val_loss: 0.3685 - val_accuracy: 0.7096\n",
    "Epoch 4/200\n",
    "1594/1594 [==============================] - 675s 424ms/step - loss: 0.4047 - accuracy: 0.5986 - val_loss: 0.3522 - val_accuracy: 0.7266\n",
    "Epoch 5/200\n",
    "1594/1594 [==============================] - 677s 425ms/step - loss: 0.4017 - accuracy: 0.6066 - val_loss: 0.3584 - val_accuracy: 0.6285\n",
    "Epoch 6/200\n",
    "1594/1594 [==============================] - 680s 426ms/step - loss: 0.3944 - accuracy: 0.6007 - val_loss: 0.3399 - val_accuracy: 0.7321\n",
    "Epoch 7/200\n",
    "1594/1594 [==============================] - 677s 425ms/step - loss: 0.3930 - accuracy: 0.5994 - val_loss: 0.3483 - val_accuracy: 0.6939\n",
    "Epoch 8/200\n",
    "1594/1594 [==============================] - 681s 428ms/step - loss: 0.3778 - accuracy: 0.6002 - val_loss: 0.3375 - val_accuracy: 0.7102\n",
    "Epoch 9/200\n",
    "1594/1594 [==============================] - 682s 428ms/step - loss: 0.3712 - accuracy: 0.6262 - val_loss: 0.3344 - val_accuracy: 0.7065\n",
    "Epoch 10/200\n",
    "1594/1594 [==============================] - 682s 428ms/step - loss: 0.3761 - accuracy: 0.6200 - val_loss: 0.3231 - val_accuracy: 0.6574\n",
    "Epoch 11/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3681 - accuracy: 0.6196 - val_loss: 0.3071 - val_accuracy: 0.7128\n",
    "Epoch 12/200\n",
    "1594/1594 [==============================] - 682s 428ms/step - loss: 0.3685 - accuracy: 0.6353 - val_loss: 0.3231 - val_accuracy: 0.7071\n",
    "Epoch 13/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3633 - accuracy: 0.6320 - val_loss: 0.3123 - val_accuracy: 0.7547\n",
    "Epoch 14/200\n",
    "1594/1594 [==============================] - 681s 427ms/step - loss: 0.3660 - accuracy: 0.6298 - val_loss: 0.3041 - val_accuracy: 0.7511\n",
    "Epoch 15/200\n",
    "1594/1594 [==============================] - 680s 427ms/step - loss: 0.3637 - accuracy: 0.6320 - val_loss: 0.3317 - val_accuracy: 0.7234\n",
    "Epoch 16/200\n",
    "1594/1594 [==============================] - 680s 427ms/step - loss: 0.3542 - accuracy: 0.6443 - val_loss: 0.3013 - val_accuracy: 0.7555\n",
    "Epoch 17/200\n",
    "1594/1594 [==============================] - 681s 427ms/step - loss: 0.3588 - accuracy: 0.6344 - val_loss: 0.3401 - val_accuracy: 0.6738\n",
    "Epoch 18/200\n",
    "1594/1594 [==============================] - 685s 429ms/step - loss: 0.3571 - accuracy: 0.6454 - val_loss: 0.2975 - val_accuracy: 0.7656\n",
    "Epoch 19/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3537 - accuracy: 0.6118 - val_loss: 0.3025 - val_accuracy: 0.7656\n",
    "Epoch 20/200\n",
    "1594/1594 [==============================] - 683s 429ms/step - loss: 0.3497 - accuracy: 0.6371 - val_loss: 0.3097 - val_accuracy: 0.7612\n",
    "Epoch 21/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3514 - accuracy: 0.6313 - val_loss: 0.3035 - val_accuracy: 0.7541\n",
    "Epoch 22/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3494 - accuracy: 0.6452 - val_loss: 0.3046 - val_accuracy: 0.7442\n",
    "Epoch 23/200\n",
    "1594/1594 [==============================] - 680s 427ms/step - loss: 0.3466 - accuracy: 0.6388 - val_loss: 0.3008 - val_accuracy: 0.7467\n",
    "Epoch 24/200\n",
    "1594/1594 [==============================] - 686s 430ms/step - loss: 0.3495 - accuracy: 0.6174 - val_loss: 0.2940 - val_accuracy: 0.7756\n",
    "Epoch 25/200\n",
    "1594/1594 [==============================] - 682s 428ms/step - loss: 0.3473 - accuracy: 0.6477 - val_loss: 0.2984 - val_accuracy: 0.7555\n",
    "Epoch 26/200\n",
    "1594/1594 [==============================] - 681s 427ms/step - loss: 0.3469 - accuracy: 0.6226 - val_loss: 0.3021 - val_accuracy: 0.7618\n",
    "Epoch 27/200\n",
    "1594/1594 [==============================] - 688s 431ms/step - loss: 0.3447 - accuracy: 0.6593 - val_loss: 0.3037 - val_accuracy: 0.7536\n",
    "Epoch 28/200\n",
    "1594/1594 [==============================] - 681s 427ms/step - loss: 0.3460 - accuracy: 0.6134 - val_loss: 0.2961 - val_accuracy: 0.7498\n",
    "Epoch 29/200\n",
    "1594/1594 [==============================] - 683s 429ms/step - loss: 0.3409 - accuracy: 0.6239 - val_loss: 0.2877 - val_accuracy: 0.7805\n",
    "Epoch 30/200\n",
    "1594/1594 [==============================] - 686s 431ms/step - loss: 0.3399 - accuracy: 0.6360 - val_loss: 0.3143 - val_accuracy: 0.7354\n",
    "Epoch 31/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.3396 - accuracy: 0.6258 - val_loss: 0.2969 - val_accuracy: 0.7586\n",
    "Epoch 32/200\n",
    "1594/1594 [==============================] - 683s 429ms/step - loss: 0.3366 - accuracy: 0.6487 - val_loss: 0.2974 - val_accuracy: 0.7630\n",
    "Epoch 33/200\n",
    "1594/1594 [==============================] - 683s 428ms/step - loss: 0.3377 - accuracy: 0.6344 - val_loss: 0.3097 - val_accuracy: 0.7492\n",
    "Epoch 34/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3366 - accuracy: 0.6578 - val_loss: 0.3081 - val_accuracy: 0.7549\n",
    "Epoch 35/200\n",
    "1594/1594 [==============================] - 686s 430ms/step - loss: 0.3319 - accuracy: 0.6269 - val_loss: 0.2981 - val_accuracy: 0.7511\n",
    "Epoch 36/200\n",
    "1594/1594 [==============================] - 681s 427ms/step - loss: 0.3404 - accuracy: 0.6058 - val_loss: 0.2864 - val_accuracy: 0.7717\n",
    "Epoch 37/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3378 - accuracy: 0.6239 - val_loss: 0.2947 - val_accuracy: 0.7612\n",
    "Epoch 38/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3357 - accuracy: 0.6378 - val_loss: 0.3006 - val_accuracy: 0.7542\n",
    "Epoch 39/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3350 - accuracy: 0.6355 - val_loss: 0.2886 - val_accuracy: 0.7762\n",
    "Epoch 40/200\n",
    "1594/1594 [==============================] - 694s 435ms/step - loss: 0.3312 - accuracy: 0.6247 - val_loss: 0.2854 - val_accuracy: 0.7511\n",
    "Epoch 41/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3302 - accuracy: 0.6254 - val_loss: 0.2943 - val_accuracy: 0.7700\n",
    "Epoch 42/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3329 - accuracy: 0.6502 - val_loss: 0.2900 - val_accuracy: 0.7599\n",
    "Epoch 43/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3345 - accuracy: 0.6404 - val_loss: 0.2750 - val_accuracy: 0.6556\n",
    "Epoch 44/200\n",
    "1594/1594 [==============================] - 683s 429ms/step - loss: 0.3316 - accuracy: 0.6156 - val_loss: 0.3012 - val_accuracy: 0.7484\n",
    "Epoch 45/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3320 - accuracy: 0.6487 - val_loss: 0.2917 - val_accuracy: 0.7542\n",
    "Epoch 46/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3354 - accuracy: 0.6564 - val_loss: 0.2886 - val_accuracy: 0.7718\n",
    "Epoch 47/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3276 - accuracy: 0.6369 - val_loss: 0.2897 - val_accuracy: 0.7492\n",
    "Epoch 48/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3308 - accuracy: 0.6250 - val_loss: 0.2998 - val_accuracy: 0.7574\n",
    "Epoch 49/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3281 - accuracy: 0.6325 - val_loss: 0.2845 - val_accuracy: 0.7718\n",
    "Epoch 50/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3312 - accuracy: 0.6378 - val_loss: 0.2815 - val_accuracy: 0.7197\n",
    "Epoch 51/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3306 - accuracy: 0.6025 - val_loss: 0.2835 - val_accuracy: 0.7762\n",
    "Epoch 52/200\n",
    "1594/1594 [==============================] - 686s 430ms/step - loss: 0.3322 - accuracy: 0.6016 - val_loss: 0.2741 - val_accuracy: 0.6421\n",
    "Epoch 53/200\n",
    "1594/1594 [==============================] - 686s 430ms/step - loss: 0.3309 - accuracy: 0.6466 - val_loss: 0.2810 - val_accuracy: 0.7687\n",
    "Epoch 54/200\n",
    "1594/1594 [==============================] - 689s 432ms/step - loss: 0.3256 - accuracy: 0.6570 - val_loss: 0.2829 - val_accuracy: 0.7876\n",
    "Epoch 55/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3255 - accuracy: 0.6212 - val_loss: 0.2853 - val_accuracy: 0.7656\n",
    "Epoch 56/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3257 - accuracy: 0.6597 - val_loss: 0.2802 - val_accuracy: 0.7775\n",
    "Epoch 57/200\n",
    "1594/1594 [==============================] - 686s 430ms/step - loss: 0.3305 - accuracy: 0.6270 - val_loss: 0.3057 - val_accuracy: 0.7580\n",
    "Epoch 58/200\n",
    "1594/1594 [==============================] - 691s 433ms/step - loss: 0.3230 - accuracy: 0.6451 - val_loss: 0.2824 - val_accuracy: 0.7813\n",
    "Epoch 59/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3256 - accuracy: 0.6371 - val_loss: 0.2834 - val_accuracy: 0.6849\n",
    "Epoch 60/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3194 - accuracy: 0.6057 - val_loss: 0.2933 - val_accuracy: 0.7649\n",
    "Epoch 61/200\n",
    "1594/1594 [==============================] - 686s 430ms/step - loss: 0.3229 - accuracy: 0.6231 - val_loss: 0.2749 - val_accuracy: 0.5864\n",
    "Epoch 62/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3211 - accuracy: 0.6239 - val_loss: 0.2826 - val_accuracy: 0.7555\n",
    "Epoch 63/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3257 - accuracy: 0.6338 - val_loss: 0.2797 - val_accuracy: 0.7442\n",
    "Epoch 64/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3232 - accuracy: 0.6388 - val_loss: 0.3042 - val_accuracy: 0.6788\n",
    "Epoch 65/200\n",
    "1594/1594 [==============================] - 689s 433ms/step - loss: 0.3229 - accuracy: 0.6325 - val_loss: 0.2750 - val_accuracy: 0.6235\n",
    "Epoch 66/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3213 - accuracy: 0.6322 - val_loss: 0.2944 - val_accuracy: 0.7172\n",
    "Epoch 67/200\n",
    "1594/1594 [==============================] - 686s 430ms/step - loss: 0.3200 - accuracy: 0.6140 - val_loss: 0.2864 - val_accuracy: 0.5403\n",
    "Epoch 68/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3167 - accuracy: 0.6360 - val_loss: 0.2736 - val_accuracy: 0.7700\n",
    "Epoch 69/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3197 - accuracy: 0.6382 - val_loss: 0.2764 - val_accuracy: 0.7700\n",
    "Epoch 70/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3187 - accuracy: 0.6451 - val_loss: 0.2833 - val_accuracy: 0.7656\n",
    "Epoch 71/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3197 - accuracy: 0.6344 - val_loss: 0.2715 - val_accuracy: 0.7209\n",
    "Epoch 72/200\n",
    "1594/1594 [==============================] - 683s 428ms/step - loss: 0.3234 - accuracy: 0.6380 - val_loss: 0.3020 - val_accuracy: 0.7593\n",
    "Epoch 73/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3185 - accuracy: 0.6377 - val_loss: 0.2955 - val_accuracy: 0.7800\n",
    "Epoch 74/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3160 - accuracy: 0.6382 - val_loss: 0.2730 - val_accuracy: 0.7373\n",
    "Epoch 75/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3194 - accuracy: 0.6333 - val_loss: 0.2949 - val_accuracy: 0.7132\n",
    "Epoch 76/200\n",
    "1594/1594 [==============================] - 688s 431ms/step - loss: 0.3220 - accuracy: 0.5961 - val_loss: 0.2788 - val_accuracy: 0.5600\n",
    "Epoch 77/200\n",
    "1594/1594 [==============================] - 683s 428ms/step - loss: 0.3234 - accuracy: 0.6137 - val_loss: 0.2713 - val_accuracy: 0.7800\n",
    "Epoch 78/200\n",
    "1594/1594 [==============================] - 683s 428ms/step - loss: 0.3158 - accuracy: 0.6458 - val_loss: 0.2844 - val_accuracy: 0.7498\n",
    "Epoch 79/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3172 - accuracy: 0.6510 - val_loss: 0.2743 - val_accuracy: 0.7649\n",
    "Epoch 80/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.3150 - accuracy: 0.6154 - val_loss: 0.2857 - val_accuracy: 0.6989\n",
    "Epoch 81/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.3204 - accuracy: 0.6104 - val_loss: 0.2879 - val_accuracy: 0.7580\n",
    "Epoch 82/200\n",
    "1594/1594 [==============================] - 682s 428ms/step - loss: 0.3150 - accuracy: 0.6325 - val_loss: 0.2653 - val_accuracy: 0.7792\n",
    "Epoch 83/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3129 - accuracy: 0.6441 - val_loss: 0.2733 - val_accuracy: 0.7549\n",
    "Epoch 84/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3154 - accuracy: 0.6173 - val_loss: 0.2665 - val_accuracy: 0.7876\n",
    "Epoch 85/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3158 - accuracy: 0.6388 - val_loss: 0.2929 - val_accuracy: 0.7058\n",
    "Epoch 86/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3187 - accuracy: 0.6311 - val_loss: 0.2680 - val_accuracy: 0.8096\n",
    "Epoch 87/200\n",
    "1594/1594 [==============================] - 689s 433ms/step - loss: 0.3153 - accuracy: 0.6482 - val_loss: 0.2747 - val_accuracy: 0.6380\n",
    "Epoch 88/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3152 - accuracy: 0.6140 - val_loss: 0.3107 - val_accuracy: 0.6845\n",
    "Epoch 89/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3159 - accuracy: 0.6601 - val_loss: 0.2763 - val_accuracy: 0.7725\n",
    "Epoch 90/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3054 - accuracy: 0.6267 - val_loss: 0.2800 - val_accuracy: 0.7308\n",
    "Epoch 91/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3176 - accuracy: 0.6419 - val_loss: 0.2697 - val_accuracy: 0.7838\n",
    "Epoch 92/200\n",
    "1594/1594 [==============================] - 686s 431ms/step - loss: 0.3161 - accuracy: 0.6424 - val_loss: 0.2726 - val_accuracy: 0.7832\n",
    "Epoch 93/200\n",
    "1594/1594 [==============================] - 693s 435ms/step - loss: 0.3111 - accuracy: 0.6382 - val_loss: 0.2809 - val_accuracy: 0.7819\n",
    "Epoch 94/200\n",
    "1594/1594 [==============================] - 685s 430ms/step - loss: 0.3164 - accuracy: 0.6273 - val_loss: 0.2720 - val_accuracy: 0.5933\n",
    "Epoch 95/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.3119 - accuracy: 0.6116 - val_loss: 0.2863 - val_accuracy: 0.5223\n",
    "Epoch 96/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3123 - accuracy: 0.6466 - val_loss: 0.2670 - val_accuracy: 0.7498\n",
    "Epoch 97/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3098 - accuracy: 0.6206 - val_loss: 0.2852 - val_accuracy: 0.7712\n",
    "Epoch 98/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3177 - accuracy: 0.6308 - val_loss: 0.2712 - val_accuracy: 0.6767\n",
    "Epoch 99/200\n",
    "1594/1594 [==============================] - 684s 429ms/step - loss: 0.3138 - accuracy: 0.6196 - val_loss: 0.2770 - val_accuracy: 0.7769\n",
    "Epoch 100/200\n",
    "1594/1594 [==============================] - 686s 431ms/step - loss: 0.3097 - accuracy: 0.6549 - val_loss: 0.2798 - val_accuracy: 0.7605\n",
    "Epoch 101/200\n",
    "1594/1594 [==============================] - 702s 441ms/step - loss: 0.3144 - accuracy: 0.6435 - val_loss: 0.2846 - val_accuracy: 0.7310\n",
    "Epoch 102/200\n",
    "1594/1594 [==============================] - 711s 446ms/step - loss: 0.3083 - accuracy: 0.6291 - val_loss: 0.2708 - val_accuracy: 0.7869\n",
    "Epoch 103/200\n",
    "1594/1594 [==============================] - 718s 451ms/step - loss: 0.3148 - accuracy: 0.6432 - val_loss: 0.2692 - val_accuracy: 0.7806\n",
    "Epoch 104/200\n",
    "1594/1594 [==============================] - 715s 449ms/step - loss: 0.3110 - accuracy: 0.6123 - val_loss: 0.2708 - val_accuracy: 0.7360\n",
    "Epoch 105/200\n",
    "1594/1594 [==============================] - 716s 449ms/step - loss: 0.3108 - accuracy: 0.6338 - val_loss: 0.2682 - val_accuracy: 0.6660\n",
    "Epoch 106/200\n",
    "1594/1594 [==============================] - 714s 448ms/step - loss: 0.3144 - accuracy: 0.6322 - val_loss: 0.2655 - val_accuracy: 0.5757\n",
    "Epoch 107/200\n",
    "1594/1594 [==============================] - 719s 451ms/step - loss: 0.3058 - accuracy: 0.6185 - val_loss: 0.2809 - val_accuracy: 0.7775\n",
    "Epoch 108/200\n",
    "1594/1594 [==============================] - 714s 448ms/step - loss: 0.3074 - accuracy: 0.6458 - val_loss: 0.2779 - val_accuracy: 0.6430\n",
    "Epoch 109/200\n",
    "1594/1594 [==============================] - 713s 448ms/step - loss: 0.3080 - accuracy: 0.6157 - val_loss: 0.2899 - val_accuracy: 0.7109\n",
    "Epoch 110/200\n",
    "1594/1594 [==============================] - 709s 445ms/step - loss: 0.3086 - accuracy: 0.6367 - val_loss: 0.2739 - val_accuracy: 0.7568\n",
    "Epoch 111/200\n",
    "1594/1594 [==============================] - 705s 442ms/step - loss: 0.3103 - accuracy: 0.6429 - val_loss: 0.2728 - val_accuracy: 0.7888\n",
    "Epoch 112/200\n",
    "1594/1594 [==============================] - 698s 438ms/step - loss: 0.3132 - accuracy: 0.6289 - val_loss: 0.2722 - val_accuracy: 0.7656\n",
    "Epoch 113/200\n",
    "1594/1594 [==============================] - 704s 442ms/step - loss: 0.3065 - accuracy: 0.6495 - val_loss: 0.2653 - val_accuracy: 0.7824\n",
    "Epoch 114/200\n",
    "1594/1594 [==============================] - 703s 441ms/step - loss: 0.3080 - accuracy: 0.6393 - val_loss: 0.2749 - val_accuracy: 0.7819\n",
    "Epoch 115/200\n",
    "1594/1594 [==============================] - 705s 442ms/step - loss: 0.3148 - accuracy: 0.6297 - val_loss: 0.2659 - val_accuracy: 0.7945\n",
    "Epoch 116/200\n",
    "1594/1594 [==============================] - 706s 443ms/step - loss: 0.3084 - accuracy: 0.6295 - val_loss: 0.2717 - val_accuracy: 0.7291\n",
    "Epoch 117/200\n",
    "1594/1594 [==============================] - 711s 446ms/step - loss: 0.3092 - accuracy: 0.6355 - val_loss: 0.2686 - val_accuracy: 0.7467\n",
    "Epoch 118/200\n",
    "1594/1594 [==============================] - 711s 446ms/step - loss: 0.3102 - accuracy: 0.6355 - val_loss: 0.2799 - val_accuracy: 0.6329\n",
    "Epoch 119/200\n",
    "1594/1594 [==============================] - 704s 442ms/step - loss: 0.3049 - accuracy: 0.6306 - val_loss: 0.2775 - val_accuracy: 0.7046\n",
    "Epoch 120/200\n",
    "1594/1594 [==============================] - 717s 450ms/step - loss: 0.3082 - accuracy: 0.6460 - val_loss: 0.2700 - val_accuracy: 0.7976\n",
    "Epoch 121/200\n",
    "1594/1594 [==============================] - 705s 442ms/step - loss: 0.3038 - accuracy: 0.6104 - val_loss: 0.2628 - val_accuracy: 0.6233\n",
    "Epoch 122/200\n",
    "1594/1594 [==============================] - 704s 442ms/step - loss: 0.3056 - accuracy: 0.6245 - val_loss: 0.2645 - val_accuracy: 0.6631\n",
    "Epoch 123/200\n",
    "1594/1594 [==============================] - 712s 447ms/step - loss: 0.3054 - accuracy: 0.6415 - val_loss: 0.2760 - val_accuracy: 0.7511\n",
    "Epoch 124/200\n",
    "1594/1594 [==============================] - 755s 474ms/step - loss: 0.3084 - accuracy: 0.6314 - val_loss: 0.2802 - val_accuracy: 0.7599\n",
    "Epoch 125/200\n",
    "1594/1594 [==============================] - 714s 448ms/step - loss: 0.3052 - accuracy: 0.6201 - val_loss: 0.2631 - val_accuracy: 0.6348\n",
    "Epoch 126/200\n",
    "1594/1594 [==============================] - 696s 437ms/step - loss: 0.3116 - accuracy: 0.6138 - val_loss: 0.2675 - val_accuracy: 0.7762\n",
    "Epoch 127/200\n",
    "1594/1594 [==============================] - 701s 440ms/step - loss: 0.3047 - accuracy: 0.6204 - val_loss: 0.2921 - val_accuracy: 0.7568\n",
    "Epoch 128/200\n",
    "1594/1594 [==============================] - 715s 449ms/step - loss: 0.3062 - accuracy: 0.6182 - val_loss: 0.2836 - val_accuracy: 0.7434\n",
    "Epoch 129/200\n",
    "1594/1594 [==============================] - 701s 440ms/step - loss: 0.3146 - accuracy: 0.6104 - val_loss: 0.2685 - val_accuracy: 0.7542\n",
    "Epoch 130/200\n",
    "1594/1594 [==============================] - 709s 445ms/step - loss: 0.3042 - accuracy: 0.6255 - val_loss: 0.2770 - val_accuracy: 0.7731\n",
    "Epoch 131/200\n",
    "1594/1594 [==============================] - 706s 443ms/step - loss: 0.3027 - accuracy: 0.6361 - val_loss: 0.2548 - val_accuracy: 0.7725\n",
    "Epoch 132/200\n",
    "1594/1594 [==============================] - 706s 443ms/step - loss: 0.3079 - accuracy: 0.6273 - val_loss: 0.2667 - val_accuracy: 0.7172\n",
    "Epoch 133/200\n",
    "1594/1594 [==============================] - 705s 443ms/step - loss: 0.3042 - accuracy: 0.6421 - val_loss: 0.2785 - val_accuracy: 0.6675\n",
    "Epoch 134/200\n",
    "1594/1594 [==============================] - 706s 443ms/step - loss: 0.3100 - accuracy: 0.6121 - val_loss: 0.2619 - val_accuracy: 0.6323\n",
    "Epoch 135/200\n",
    "1594/1594 [==============================] - 698s 438ms/step - loss: 0.3008 - accuracy: 0.6062 - val_loss: 0.2705 - val_accuracy: 0.6700\n",
    "Epoch 136/200\n",
    "1594/1594 [==============================] - 706s 443ms/step - loss: 0.3101 - accuracy: 0.6515 - val_loss: 0.2665 - val_accuracy: 0.6774\n",
    "Epoch 137/200\n",
    "1594/1594 [==============================] - 700s 439ms/step - loss: 0.3086 - accuracy: 0.6058 - val_loss: 0.2989 - val_accuracy: 0.6939\n",
    "Epoch 138/200\n",
    "1594/1594 [==============================] - 711s 446ms/step - loss: 0.3024 - accuracy: 0.6090 - val_loss: 0.2668 - val_accuracy: 0.7329\n",
    "Epoch 139/200\n",
    "1594/1594 [==============================] - 715s 449ms/step - loss: 0.3019 - accuracy: 0.6422 - val_loss: 0.2765 - val_accuracy: 0.7429\n",
    "Epoch 140/200\n",
    "1594/1594 [==============================] - 702s 441ms/step - loss: 0.3059 - accuracy: 0.6062 - val_loss: 0.2816 - val_accuracy: 0.6562\n",
    "Epoch 141/200\n",
    "1594/1594 [==============================] - 704s 442ms/step - loss: 0.3053 - accuracy: 0.6167 - val_loss: 0.2611 - val_accuracy: 0.7241\n",
    "Epoch 142/200\n",
    "1594/1594 [==============================] - 711s 446ms/step - loss: 0.3021 - accuracy: 0.6149 - val_loss: 0.2653 - val_accuracy: 0.7517\n",
    "Epoch 143/200\n",
    "1594/1594 [==============================] - 714s 448ms/step - loss: 0.3054 - accuracy: 0.6229 - val_loss: 0.2735 - val_accuracy: 0.6769\n",
    "Epoch 144/200\n",
    "1594/1594 [==============================] - 711s 446ms/step - loss: 0.3042 - accuracy: 0.6204 - val_loss: 0.2946 - val_accuracy: 0.7516\n",
    "Epoch 145/200\n",
    "1594/1594 [==============================] - 701s 440ms/step - loss: 0.3003 - accuracy: 0.6393 - val_loss: 0.2707 - val_accuracy: 0.6562\n",
    "Epoch 146/200\n",
    "1594/1594 [==============================] - 717s 450ms/step - loss: 0.3058 - accuracy: 0.6220 - val_loss: 0.2746 - val_accuracy: 0.7819\n",
    "Epoch 147/200\n",
    "1594/1594 [==============================] - 700s 440ms/step - loss: 0.3044 - accuracy: 0.6358 - val_loss: 0.2831 - val_accuracy: 0.7637\n",
    "Epoch 148/200\n",
    "1594/1594 [==============================] - 707s 444ms/step - loss: 0.3003 - accuracy: 0.6382 - val_loss: 0.2562 - val_accuracy: 0.7901\n",
    "Epoch 149/200\n",
    "1594/1594 [==============================] - 699s 439ms/step - loss: 0.3028 - accuracy: 0.6501 - val_loss: 0.2676 - val_accuracy: 0.7888\n",
    "Epoch 150/200\n",
    "1594/1594 [==============================] - 701s 440ms/step - loss: 0.3024 - accuracy: 0.6529 - val_loss: 0.2689 - val_accuracy: 0.7649\n",
    "Epoch 151/200\n",
    "1594/1594 [==============================] - 703s 441ms/step - loss: 0.3054 - accuracy: 0.6179 - val_loss: 0.2703 - val_accuracy: 0.7560\n",
    "Epoch 152/200\n",
    "1594/1594 [==============================] - 702s 440ms/step - loss: 0.2994 - accuracy: 0.6149 - val_loss: 0.2557 - val_accuracy: 0.6568\n",
    "Epoch 153/200\n",
    "1594/1594 [==============================] - 710s 445ms/step - loss: 0.3039 - accuracy: 0.6358 - val_loss: 0.2686 - val_accuracy: 0.7536\n",
    "Epoch 154/200\n",
    "1594/1594 [==============================] - 702s 440ms/step - loss: 0.2993 - accuracy: 0.6231 - val_loss: 0.2761 - val_accuracy: 0.7769\n",
    "Epoch 155/200\n",
    "1594/1594 [==============================] - 715s 448ms/step - loss: 0.2993 - accuracy: 0.6102 - val_loss: 0.2987 - val_accuracy: 0.6292\n",
    "Epoch 156/200\n",
    "1594/1594 [==============================] - 699s 439ms/step - loss: 0.3000 - accuracy: 0.6079 - val_loss: 0.2734 - val_accuracy: 0.5889\n",
    "Epoch 157/200\n",
    "1594/1594 [==============================] - 689s 433ms/step - loss: 0.2988 - accuracy: 0.6157 - val_loss: 0.2621 - val_accuracy: 0.7366\n",
    "Epoch 158/200\n",
    "1594/1594 [==============================] - 683s 429ms/step - loss: 0.3010 - accuracy: 0.6124 - val_loss: 0.2583 - val_accuracy: 0.7674\n",
    "Epoch 159/200\n",
    "1594/1594 [==============================] - 695s 436ms/step - loss: 0.2956 - accuracy: 0.6311 - val_loss: 0.2830 - val_accuracy: 0.6371\n",
    "Epoch 160/200\n",
    "1594/1594 [==============================] - 699s 438ms/step - loss: 0.3013 - accuracy: 0.6250 - val_loss: 0.2599 - val_accuracy: 0.7901\n",
    "Epoch 161/200\n",
    "1594/1594 [==============================] - 693s 435ms/step - loss: 0.3041 - accuracy: 0.6077 - val_loss: 0.2763 - val_accuracy: 0.7800\n",
    "Epoch 162/200\n",
    "1594/1594 [==============================] - 689s 432ms/step - loss: 0.3040 - accuracy: 0.6488 - val_loss: 0.2735 - val_accuracy: 0.7366\n",
    "Epoch 163/200\n",
    "1594/1594 [==============================] - 692s 434ms/step - loss: 0.2964 - accuracy: 0.6120 - val_loss: 0.2825 - val_accuracy: 0.5405\n",
    "Epoch 164/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.3015 - accuracy: 0.6104 - val_loss: 0.2602 - val_accuracy: 0.7291\n",
    "Epoch 165/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.3056 - accuracy: 0.6118 - val_loss: 0.2617 - val_accuracy: 0.5959\n",
    "Epoch 166/200\n",
    "1594/1594 [==============================] - 689s 432ms/step - loss: 0.3012 - accuracy: 0.6248 - val_loss: 0.2776 - val_accuracy: 0.5739\n",
    "Epoch 167/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.3006 - accuracy: 0.6256 - val_loss: 0.2614 - val_accuracy: 0.7283\n",
    "Epoch 168/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.2983 - accuracy: 0.6062 - val_loss: 0.2783 - val_accuracy: 0.7549\n",
    "Epoch 169/200\n",
    "1594/1594 [==============================] - 702s 440ms/step - loss: 0.2986 - accuracy: 0.6134 - val_loss: 0.2676 - val_accuracy: 0.6952\n",
    "Epoch 170/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.3066 - accuracy: 0.6126 - val_loss: 0.2640 - val_accuracy: 0.5896\n",
    "Epoch 171/200\n",
    "1594/1594 [==============================] - 694s 435ms/step - loss: 0.2954 - accuracy: 0.6394 - val_loss: 0.2742 - val_accuracy: 0.6788\n",
    "Epoch 172/200\n",
    "1594/1594 [==============================] - 696s 437ms/step - loss: 0.3020 - accuracy: 0.6090 - val_loss: 0.2529 - val_accuracy: 0.5820\n",
    "Epoch 173/200\n",
    "1594/1594 [==============================] - 689s 433ms/step - loss: 0.2981 - accuracy: 0.6421 - val_loss: 0.2702 - val_accuracy: 0.7291\n",
    "Epoch 174/200\n",
    "1594/1594 [==============================] - 689s 432ms/step - loss: 0.2950 - accuracy: 0.6341 - val_loss: 0.2632 - val_accuracy: 0.7509\n",
    "Epoch 175/200\n",
    "1594/1594 [==============================] - 692s 434ms/step - loss: 0.2989 - accuracy: 0.6325 - val_loss: 0.2591 - val_accuracy: 0.6361\n",
    "Epoch 176/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3062 - accuracy: 0.6546 - val_loss: 0.2719 - val_accuracy: 0.7850\n",
    "Epoch 177/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.3006 - accuracy: 0.6096 - val_loss: 0.2658 - val_accuracy: 0.7209\n",
    "Epoch 178/200\n",
    "1594/1594 [==============================] - 692s 434ms/step - loss: 0.2986 - accuracy: 0.6244 - val_loss: 0.2622 - val_accuracy: 0.7561\n",
    "Epoch 179/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.3019 - accuracy: 0.6341 - val_loss: 0.2710 - val_accuracy: 0.7618\n",
    "Epoch 180/200\n",
    "1594/1594 [==============================] - 689s 433ms/step - loss: 0.3021 - accuracy: 0.6295 - val_loss: 0.2640 - val_accuracy: 0.7046\n",
    "Epoch 181/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.2951 - accuracy: 0.6317 - val_loss: 0.2615 - val_accuracy: 0.7398\n",
    "Epoch 182/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.2996 - accuracy: 0.6377 - val_loss: 0.2740 - val_accuracy: 0.7591\n",
    "Epoch 183/200\n",
    "1594/1594 [==============================] - 699s 439ms/step - loss: 0.3005 - accuracy: 0.6446 - val_loss: 0.2871 - val_accuracy: 0.7649\n",
    "Epoch 184/200\n",
    "1594/1594 [==============================] - 692s 434ms/step - loss: 0.3053 - accuracy: 0.6319 - val_loss: 0.2560 - val_accuracy: 0.6518\n",
    "Epoch 185/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.2991 - accuracy: 0.6280 - val_loss: 0.2639 - val_accuracy: 0.7825\n",
    "Epoch 186/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.3014 - accuracy: 0.6582 - val_loss: 0.2769 - val_accuracy: 0.6021\n",
    "Epoch 187/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.3034 - accuracy: 0.6036 - val_loss: 0.2746 - val_accuracy: 0.7674\n",
    "Epoch 188/200\n",
    "1594/1594 [==============================] - 691s 433ms/step - loss: 0.3019 - accuracy: 0.6302 - val_loss: 0.2583 - val_accuracy: 0.5940\n",
    "Epoch 189/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.2979 - accuracy: 0.6366 - val_loss: 0.2714 - val_accuracy: 0.7769\n",
    "Epoch 190/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.2990 - accuracy: 0.6298 - val_loss: 0.2587 - val_accuracy: 0.6107\n",
    "Epoch 191/200\n",
    "1594/1594 [==============================] - 691s 433ms/step - loss: 0.2957 - accuracy: 0.6105 - val_loss: 0.2629 - val_accuracy: 0.7863\n",
    "Epoch 192/200\n",
    "1594/1594 [==============================] - 694s 435ms/step - loss: 0.2980 - accuracy: 0.6344 - val_loss: 0.2688 - val_accuracy: 0.7354\n",
    "Epoch 193/200\n",
    "1594/1594 [==============================] - 691s 434ms/step - loss: 0.2931 - accuracy: 0.6055 - val_loss: 0.2734 - val_accuracy: 0.7184\n",
    "Epoch 194/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3064 - accuracy: 0.6413 - val_loss: 0.2572 - val_accuracy: 0.7568\n",
    "Epoch 195/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.2999 - accuracy: 0.6231 - val_loss: 0.2708 - val_accuracy: 0.6882\n",
    "Epoch 196/200\n",
    "1594/1594 [==============================] - 689s 433ms/step - loss: 0.3001 - accuracy: 0.6102 - val_loss: 0.2658 - val_accuracy: 0.7926\n",
    "Epoch 197/200\n",
    "1594/1594 [==============================] - 690s 433ms/step - loss: 0.2995 - accuracy: 0.6363 - val_loss: 0.2622 - val_accuracy: 0.7509\n",
    "Epoch 198/200\n",
    "1594/1594 [==============================] - 692s 434ms/step - loss: 0.2948 - accuracy: 0.6226 - val_loss: 0.2732 - val_accuracy: 0.7731\n",
    "Epoch 199/200\n",
    "1594/1594 [==============================] - 687s 431ms/step - loss: 0.2948 - accuracy: 0.5985 - val_loss: 0.2580 - val_accuracy: 0.6354\n",
    "Epoch 200/200\n",
    "1594/1594 [==============================] - 688s 432ms/step - loss: 0.3043 - accuracy: 0.6104 - val_loss: 0.2594 - val_accuracy: 0.8020\n",
    "\"\"\"\n",
    "\n",
    "# Parse the output and get the history\n",
    "history = parse_training_output(training_output)\n",
    "\n",
    "# Now you can use the history dictionary for plotting or analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
